{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport math\nimport copy\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU is not available. Using CPU.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"markdown","source":"## Multihead Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        if (d_model % num_heads != 0):\n            raise ValueError('d_model (dimension of word embeddings) must be divisible by num_heads')\n        \n        # d_model refers to the dimension of the word vectors that we use throughout the model\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # trainable parameters for the attention\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # dot pdt and scale\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n        \n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        \n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Position Wise Feed Forward Layer\nThis is a normal Feed Forward layer. The same network is applied to every postion (or word) in the sentence. <br /> <br />\nIt contains <br />\nInput layer of shape = embedding dimension <br />\nOutput layer of shape = embedding dimension <br />\nOne hidden layer (no. of nodes = 2048 in the original paper)","metadata":{}},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff) #input layer\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Postitional Encoding\nNo RNN used in Transformer. So we need to add sense of postion of word in a sentence <br />\n$PE(pos, 2i) = sin(pos/10000^{2i/dmodel})$ <br />\n$PE(pos, 2i+1) = cos(pos/10000^{2i/dmodel})$","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        # even postions (start from index 0, jump by 2)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        \n        #odd postitions (start from index 1, jump by 2)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)] # add the pos embedding with word embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder Layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder Layer","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.max_seq_length = max_seq_length\n\n    def generate_mask(self, src, tgt):\n        src_mask = None\n        tgt_mask = None\n        if src != None:\n            src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)\n        if tgt != None:\n            tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n            seq_length = tgt.size(1)\n            nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n            tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n            \n        output = self.fc(dec_output)\n        return output\n    \n    def inference(self, src):\n        src_mask, _ = self.generate_mask(src, None)\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        # Initialize the decoder input with a start token\n        decoder_input_init = torch.tensor([[1]]).to(src.device)\n        decoder_input = decoder_input_init\n\n        output_sequence = []\n        for _ in range(self.max_seq_length):\n            #_, tgt_mask = self.generate_mask(None, decoder_input)\n            tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(decoder_input)))\n\n            dec_output = tgt_embedded\n            for dec_layer in self.decoder_layers:\n                dec_output = dec_layer(dec_output, enc_output, src_mask, None)\n\n            output = self.fc(dec_output)\n            predicted_token = torch.argmax(output, dim=-1)\n\n            #Append the predicted token to the decoder input\n            decoder_input = torch.cat((decoder_input_init, predicted_token), dim=1)\n\n        return predicted_token\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/fr-eng/fra.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names = ['English', 'French']\n\ndf = pd.read_csv(file_path, delimiter='\\t', usecols=[0, 1], names=column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_sentences = df['English'].tolist()\nfrench_sentences = df['French'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unknown token\nSometimes model may face unknown words, especially names of people, places etc. <br />\nI add some examples to the dataset. <br />\n(Here, I translate the unknown word as it is (i.e., name of people, places etc))","metadata":{}},{"cell_type":"code","source":"english_addn = [\n    \"<unk> is a good boy\",\n    \"My favourite food is <unk>\",\n    \"I live in <unk>\",\n    \"Tom lives in <unk>\",\n    \"<unk> lives in Kolkata\",\n    \"<unk> is very sick and needs to go to a doctor\",\n    \"<unk>\",\n    \"Today I cooked <unk> and it was delicious\"\n]\n\nfrench_addn = [\n    \"<unk> est un bon garçon\",\n    \"Ma nourriture préféré est <unk>\",\n    \"J'habite à <unk>\",\n    \"Tom habite à <unk>\",\n    \"<unk> habite à Kolkata\",\n    \"<unk> est très malade est a besoin d'aller chez un médecin\",\n    \"<unk>\",\n    \"Aujourd'hui j'ai cuisiné <unk> et c'était délicieux\"\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_sentences.extend(english_addn)\nfrench_sentences.extend(french_addn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{english_sentences[-1]}\\n{french_sentences[-1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lower_and_preprocess(x):\n    x = str.lower(x)\n    x = x.replace('\\u202f', ' ') # no break space with normal space\n    x = x.replace('\\xa0', ' ')\n    x = x.replace('\\u2009', ' ')\n    x = x.replace(\"'\", \"' \") # words with apostophe are seperated into 2 words\n    x = '<sos> ' + x + ' <eos>' \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_sentences = list(map(lower_and_preprocess, english_sentences))\nfrench_sentences = list(map(lower_and_preprocess, french_sentences))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_vocab = {'<pad>': 0, '<sos>': 1, '<eos>':2, '<unk>': 3}\nfrench_vocab = {'<pad>': 0, '<sos>': 1, '<eos>':2, '<unk>': 3}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer(sentences, vocab):\n    tokenized_list = []\n    for sentence in sentences:\n        tokens = sentence.strip().split() \n        encoded = [vocab.setdefault(token, len(vocab)) for token in tokens]\n        tokenized_list.append(encoded)\n    return tokenized_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer_inference(sentences, vocab):\n    tokenized_list = []\n    for sentence in sentences:\n        tokens = sentence.strip().split() \n        encoded = [vocab.get(token) or vocab.get('<unk>') for token in tokens]\n        tokenized_list.append(encoded)\n    return tokenized_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_tokenized = tokenizer(english_sentences, english_vocab)\nfrench_tokenized = tokenizer(french_sentences, french_vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just checking\nprint(f'{english_tokenized[-1]}\\n{french_tokenized[-1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Padding\nenglish_padded = pad_sequence([torch.tensor(seq) for seq in english_tokenized], batch_first=True)\nfrench_padded = pad_sequence([torch.tensor(seq) for seq in french_tokenized], batch_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_vocab_rev = {value: key for key, value in english_vocab.items()}\nfrench_vocab_rev = {value: key for key, value in french_vocab.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Dataset Creation\nclass TranslationDataset(Dataset):\n    def __init__(self, english_data, french_data):\n        self.english_data = english_data\n        self.french_data = french_data\n\n    def __len__(self):\n        return len(self.english_data)\n\n    def __getitem__(self, index):\n        return {\n            'input': self.english_data[index],\n            'target': self.french_data[index]\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Data Loaders\nbatch_size = 32\ndataset = TranslationDataset(english_padded, french_padded)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_eng = len(english_vocab)\nsize_fr = len(french_vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{size_eng},\\n{size_fr}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"french_padded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_padded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_vocab_size = size_eng\ntgt_vocab_size = size_fr\nd_model = 512\nnum_heads = 8\nnum_layers = 6\nd_ff = 2048\nmax_seq_length = 70\ndropout = 0.1\n\ntransformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.train()\n\nfor epoch in range(2):\n    batch_cnt = 0\n    tot_loss = 0\n    \n    for batch in data_loader:\n        batch_cnt += 1\n        input_batch = batch['input'].to('cuda')  # Tensor of shape (batch_size, max_english_length)\n        target_batch = batch['target'].to('cuda')  # Tensor of shape (batch_size, max_french_length)\n        # Use the batches for training your Transformer model\n        optimizer.zero_grad()\n        output = transformer(input_batch, target_batch[:, :-1])\n        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_batch[:, 1:].contiguous().view(-1))\n        loss.backward()\n        optimizer.step()\n        \n        tot_loss += loss.item()\n        if (batch_cnt % 500 == 0):\n            print(f'\\tEpoch: {epoch+1} Batch: {batch_cnt}, Loss: {loss.item()}')\n            \n    print(f\"Epoch: {epoch+1}, Loss: {tot_loss/batch_cnt}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working/transformer_state.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(transformer.state_dict(), path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transformer.load_state_dict(torch.load(path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = transformer(english_padded[90003:90004].to(device), french_padded[90003:90004].to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_res = torch.argmax(res, axis=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_res.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence(vec, mapping_dict):\n    res = ''\n    for item in vec:\n#         print(item)\n        res_tmp = mapping_dict[item.item()]\n        if (res_tmp == '<sos>'):\n            continue\n        \n        if (res_tmp == '<eos>'):\n            break\n        res += ' ' + res_tmp\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_sentence(numerical_res[0].to('cpu'), french_vocab_rev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"french_vocab_rev[83]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_sentence(english_padded[90003].to('cpu'), english_vocab_rev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_sentence(french_padded[90003].to('cpu'), french_vocab_rev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}